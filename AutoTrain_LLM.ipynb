{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "JvMRbVLEJlZT",
        "outputId": "355d4b7d-c472-425b-83be-6a3f66938452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ¤— AutoTrain LLM\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - upload train.csv to a folder named `data/`\n",
        "#@markdown - train.csv must contain a `text` column\n",
        "#@markdown - choose a project name if you wish\n",
        "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
        "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
        "#@markdown - update hyperparameters if you wish\n",
        "#@markdown - click `Runtime > Run all` or run each cell individually\n",
        "#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n",
        "\n",
        "import os\n",
        "!pip install -U autotrain-advanced > install_logs.txt\n",
        "!autotrain setup --colab > setup_logs.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "A2-_lkBS1WKA"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown #### Project Config\n",
        "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
        "project_name = 'solana-smart-contract-auditor' # @param {type:\"string\"}\n",
        "model_name = 'filipealmeida/Mistral-7B-Instruct-v0.1-sharded' # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Push to Hub?\n",
        "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
        "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
        "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
        "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
        "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "hf_token = \"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\" #@param {type:\"string\"}\n",
        "repo_id = \"ArmurAI/solana_smart_contract_auditor\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Hyperparameters\n",
        "learning_rate = 2e-4 # @param {type:\"number\"}\n",
        "num_epochs = 1 #@param {type:\"number\"}\n",
        "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "block_size = 1024 # @param {type:\"number\"}\n",
        "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
        "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "gradient_accumulation = 4 # @param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n",
        "peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\n",
        "lora_r = 16 #@param {type:\"number\"}\n",
        "lora_alpha = 32 #@param {type:\"number\"}\n",
        "lora_dropout = 0.05 #@param {type:\"number\"}\n",
        "\n",
        "os.environ[\"PROJECT_NAME\"] = project_name\n",
        "os.environ[\"MODEL_NAME\"] = model_name\n",
        "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"REPO_ID\"] = repo_id\n",
        "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
        "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
        "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
        "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
        "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
        "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
        "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
        "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
        "os.environ[\"PEFT\"] = str(peft)\n",
        "os.environ[\"QUANTIZATION\"] = str(quantization)\n",
        "os.environ[\"LORA_R\"] = str(lora_r)\n",
        "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
        "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "g3cd_ED_yXXt",
        "outputId": "25c4d11a-f075-4da2-9741-abb3edfc03f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[1mINFO    Running LLM\u001b[0m\n",
            "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.01, max_grad_norm=1.0, add_eos_token=False, block_size=1024, peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision='fp16', quantization='int4', model_max_length=1024, trainer='default', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, chat_template=None, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token='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', repo_id='ArmurAI/solana_smart_contract_auditor', push_to_hub=True, model='filipealmeida/Mistral-7B-Instruct-v0.1-sharded', project_name='solana-smart-contract-auditor', seed=42, epochs=1, gradient_accumulation=4, disable_gradient_checkpointing=False, lr=0.0002, log='none', data_path='data/', train_split='train', valid_split=None, batch_size=1, func=<function run_llm_command_factory at 0x7cbf6b94bbe0>)\u001b[0m\n",
            "> \u001b[1mINFO    Dataset: solana-smart-contract-auditor (lm_training)\n",
            "Train data: ['data//train.csv']\n",
            "Valid data: []\n",
            "Column mapping: {'text': 'text', 'rejected_text': 'rejected', 'prompt': 'prompt'}\n",
            "\u001b[0m\n",
            "\rSaving the dataset (0/1 shards):   0% 0/244 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 244/244 [00:00<00:00, 23549.95 examples/s]\rSaving the dataset (1/1 shards): 100% 244/244 [00:00<00:00, 23102.85 examples/s]\n",
            "\rSaving the dataset (0/1 shards):   0% 0/244 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 244/244 [00:00<00:00, 79223.58 examples/s]\rSaving the dataset (1/1 shards): 100% 244/244 [00:00<00:00, 74197.79 examples/s]\n",
            "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
            "> \u001b[1mINFO    {\"model\":\"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\",\"project_name\":\"solana-smart-contract-auditor\",\"data_path\":\"solana-smart-contract-auditor/autotrain-data\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":1024,\"model_max_length\":1024,\"padding\":null,\"trainer\":\"default\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":\"fp16\",\"lr\":0.0002,\"epochs\":1,\"batch_size\":1,\"warmup_ratio\":0.1,\"gradient_accumulation\":4,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.01,\"max_grad_norm\":1.0,\"seed\":42,\"chat_template\":null,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.05,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"autotrain_prompt\",\"text_column\":\"autotrain_text\",\"rejected_text_column\":\"autotrain_rejected_text\",\"push_to_hub\":true,\"repo_id\":\"ArmurAI/solana_smart_contract_auditor\",\"username\":null,\"token\":\"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\"}\u001b[0m\n",
            "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'solana-smart-contract-auditor/training_params.json']\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:44:38\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:44:38\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['autotrain_text'],\n",
            "    num_rows: 244\n",
            "})\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:44:38\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "tokenizer_config.json: 100% 1.47k/1.47k [00:00<00:00, 7.59MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 13.5MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 6.86MB/s]\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 403kB/s]\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:44:39\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mcreating training arguments...\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:44:39\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:44:39\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.66MB/s]\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:44:40\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "pytorch_model.bin.index.json: 100% 23.9k/23.9k [00:00<00:00, 76.6MB/s]\n",
            "Downloading shards:   0% 0/8 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00008.bin:   0% 0.00/1.89G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   1% 10.5M/1.89G [00:00<01:11, 26.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   1% 21.0M/1.89G [00:00<00:50, 37.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   2% 31.5M/1.89G [00:00<00:40, 46.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   2% 41.9M/1.89G [00:00<00:31, 58.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   3% 62.9M/1.89G [00:01<00:23, 79.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   4% 83.9M/1.89G [00:01<00:18, 99.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   6% 105M/1.89G [00:01<00:15, 118MB/s]  \u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   7% 126M/1.89G [00:01<00:13, 135MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   8% 147M/1.89G [00:01<00:12, 145MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:   9% 168M/1.89G [00:01<00:11, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  10% 189M/1.89G [00:01<00:10, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  11% 210M/1.89G [00:01<00:10, 168MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  12% 231M/1.89G [00:02<00:09, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  13% 252M/1.89G [00:02<00:09, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  14% 273M/1.89G [00:02<00:09, 178MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  16% 294M/1.89G [00:02<00:08, 180MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  17% 315M/1.89G [00:02<00:09, 174MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  18% 336M/1.89G [00:02<00:08, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  19% 357M/1.89G [00:02<00:08, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  20% 377M/1.89G [00:02<00:08, 179MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  21% 398M/1.89G [00:02<00:08, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  22% 419M/1.89G [00:03<00:08, 170MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  23% 440M/1.89G [00:05<00:54, 26.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  24% 461M/1.89G [00:05<00:41, 34.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  26% 482M/1.89G [00:05<00:31, 44.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  27% 503M/1.89G [00:05<00:25, 55.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  28% 524M/1.89G [00:06<00:19, 69.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  29% 545M/1.89G [00:06<00:15, 86.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  30% 566M/1.89G [00:06<00:12, 104MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  31% 587M/1.89G [00:06<00:11, 117MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  32% 608M/1.89G [00:06<00:10, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  33% 629M/1.89G [00:06<00:09, 134MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  34% 650M/1.89G [00:06<00:08, 143MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  36% 671M/1.89G [00:06<00:08, 141MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  37% 692M/1.89G [00:07<00:07, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  38% 724M/1.89G [00:07<00:06, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  39% 744M/1.89G [00:07<00:06, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  41% 765M/1.89G [00:07<00:06, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  42% 786M/1.89G [00:07<00:06, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  43% 807M/1.89G [00:07<00:06, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  44% 828M/1.89G [00:07<00:06, 170MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  45% 849M/1.89G [00:07<00:05, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  46% 870M/1.89G [00:08<00:05, 178MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  47% 891M/1.89G [00:08<00:05, 178MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  48% 912M/1.89G [00:08<00:05, 174MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  49% 933M/1.89G [00:10<00:32, 29.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  50% 954M/1.89G [00:10<00:24, 38.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  52% 975M/1.89G [00:10<00:21, 43.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  53% 996M/1.89G [00:11<00:17, 50.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  53% 1.01G/1.89G [00:11<00:16, 54.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  54% 1.02G/1.89G [00:11<00:16, 54.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  54% 1.03G/1.89G [00:11<00:16, 53.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  55% 1.04G/1.89G [00:11<00:15, 55.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  55% 1.05G/1.89G [00:11<00:14, 58.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  56% 1.06G/1.89G [00:12<00:14, 56.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  57% 1.07G/1.89G [00:12<00:13, 61.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  57% 1.08G/1.89G [00:12<00:12, 64.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  58% 1.09G/1.89G [00:12<00:11, 67.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  59% 1.11G/1.89G [00:12<00:08, 93.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  60% 1.13G/1.89G [00:12<00:06, 115MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  61% 1.15G/1.89G [00:12<00:05, 131MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  62% 1.17G/1.89G [00:13<00:05, 141MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  63% 1.20G/1.89G [00:13<00:04, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  64% 1.22G/1.89G [00:13<00:04, 162MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  65% 1.24G/1.89G [00:13<00:03, 168MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  67% 1.26G/1.89G [00:13<00:03, 173MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  68% 1.28G/1.89G [00:13<00:03, 171MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  69% 1.30G/1.89G [00:13<00:03, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  70% 1.32G/1.89G [00:13<00:03, 174MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  71% 1.34G/1.89G [00:13<00:03, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  72% 1.36G/1.89G [00:14<00:03, 173MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  73% 1.38G/1.89G [00:14<00:02, 174MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  74% 1.41G/1.89G [00:14<00:02, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  75% 1.43G/1.89G [00:14<00:02, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  77% 1.45G/1.89G [00:14<00:02, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  78% 1.47G/1.89G [00:15<00:06, 65.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  79% 1.50G/1.89G [00:15<00:04, 89.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  81% 1.53G/1.89G [00:15<00:03, 111MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  82% 1.55G/1.89G [00:15<00:02, 115MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  83% 1.57G/1.89G [00:15<00:02, 125MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  85% 1.60G/1.89G [00:16<00:01, 150MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  86% 1.63G/1.89G [00:16<00:01, 150MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  87% 1.65G/1.89G [00:16<00:01, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  89% 1.68G/1.89G [00:16<00:01, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  90% 1.70G/1.89G [00:16<00:01, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  91% 1.72G/1.89G [00:16<00:00, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  92% 1.74G/1.89G [00:16<00:00, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  93% 1.76G/1.89G [00:16<00:00, 178MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  94% 1.78G/1.89G [00:17<00:00, 180MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  95% 1.80G/1.89G [00:17<00:00, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  97% 1.82G/1.89G [00:17<00:00, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  98% 1.85G/1.89G [00:17<00:00, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin:  99% 1.87G/1.89G [00:17<00:00, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00008.bin: 100% 1.89G/1.89G [00:17<00:00, 107MB/s]\n",
            "Downloading shards:  12% 1/8 [00:17<02:05, 17.94s/it]\n",
            "pytorch_model-00002-of-00008.bin:   0% 0.00/1.95G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   1% 10.5M/1.95G [00:00<00:57, 33.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   1% 21.0M/1.95G [00:00<00:43, 43.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   2% 31.5M/1.95G [00:00<00:34, 55.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   2% 41.9M/1.95G [00:00<00:28, 66.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   3% 62.9M/1.95G [00:00<00:21, 88.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   4% 83.9M/1.95G [00:01<00:17, 108MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   5% 105M/1.95G [00:01<00:14, 127MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   6% 126M/1.95G [00:01<00:12, 144MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   8% 147M/1.95G [00:01<00:12, 143MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:   9% 168M/1.95G [00:01<00:11, 153MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  10% 189M/1.95G [00:01<00:10, 163MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  11% 210M/1.95G [00:02<00:27, 62.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  12% 241M/1.95G [00:02<00:19, 88.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  13% 262M/1.95G [00:02<00:16, 105MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  15% 283M/1.95G [00:02<00:14, 112MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  16% 304M/1.95G [00:02<00:13, 120MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  17% 325M/1.95G [00:03<00:12, 133MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  18% 357M/1.95G [00:03<00:10, 152MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  19% 377M/1.95G [00:03<00:10, 154MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  20% 398M/1.95G [00:03<00:09, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  22% 419M/1.95G [00:03<00:09, 168MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  23% 440M/1.95G [00:03<00:08, 171MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  24% 461M/1.95G [00:03<00:08, 177MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  25% 482M/1.95G [00:03<00:08, 176MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  26% 503M/1.95G [00:04<00:08, 179MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  27% 524M/1.95G [00:04<00:08, 169MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  28% 545M/1.95G [00:04<00:08, 169MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  29% 566M/1.95G [00:04<00:07, 175MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  30% 587M/1.95G [00:04<00:07, 176MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  31% 608M/1.95G [00:04<00:13, 102MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  32% 629M/1.95G [00:07<00:57, 23.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  34% 661M/1.95G [00:07<00:36, 35.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  35% 682M/1.95G [00:07<00:27, 45.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  36% 703M/1.95G [00:07<00:22, 55.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  37% 724M/1.95G [00:08<00:17, 68.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  38% 744M/1.95G [00:08<00:14, 84.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  40% 776M/1.95G [00:08<00:10, 107MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  41% 797M/1.95G [00:08<00:09, 120MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  42% 818M/1.95G [00:08<00:08, 132MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  43% 839M/1.95G [00:08<00:07, 147MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  44% 860M/1.95G [00:08<00:06, 158MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  45% 881M/1.95G [00:08<00:06, 165MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  46% 902M/1.95G [00:09<00:06, 168MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  47% 923M/1.95G [00:09<00:06, 169MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  48% 944M/1.95G [00:09<00:05, 177MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  50% 965M/1.95G [00:09<00:05, 181MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  51% 986M/1.95G [00:09<00:05, 176MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  52% 1.01G/1.95G [00:09<00:05, 176MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  53% 1.03G/1.95G [00:09<00:05, 176MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  54% 1.05G/1.95G [00:09<00:05, 177MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  55% 1.07G/1.95G [00:09<00:04, 176MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  56% 1.09G/1.95G [00:11<00:22, 38.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  57% 1.11G/1.95G [00:12<00:24, 34.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  58% 1.13G/1.95G [00:12<00:18, 43.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  59% 1.15G/1.95G [00:12<00:14, 55.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  60% 1.17G/1.95G [00:12<00:11, 68.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  61% 1.20G/1.95G [00:12<00:10, 70.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  62% 1.22G/1.95G [00:13<00:08, 86.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  64% 1.24G/1.95G [00:13<00:07, 101MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  65% 1.26G/1.95G [00:13<00:06, 112MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  66% 1.28G/1.95G [00:13<00:05, 126MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  67% 1.30G/1.95G [00:13<00:04, 141MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  68% 1.32G/1.95G [00:13<00:04, 152MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  69% 1.34G/1.95G [00:13<00:03, 162MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  70% 1.36G/1.95G [00:14<00:05, 116MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  71% 1.38G/1.95G [00:14<00:04, 133MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  72% 1.41G/1.95G [00:14<00:03, 145MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  73% 1.43G/1.95G [00:14<00:03, 152MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  74% 1.45G/1.95G [00:14<00:03, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  75% 1.47G/1.95G [00:14<00:02, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  77% 1.49G/1.95G [00:14<00:02, 169MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  78% 1.51G/1.95G [00:14<00:02, 177MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  79% 1.53G/1.95G [00:15<00:02, 180MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  80% 1.55G/1.95G [00:15<00:02, 169MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  81% 1.57G/1.95G [00:17<00:13, 27.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  82% 1.60G/1.95G [00:17<00:08, 41.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  84% 1.63G/1.95G [00:17<00:06, 52.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  85% 1.65G/1.95G [00:17<00:04, 65.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  86% 1.67G/1.95G [00:17<00:03, 77.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  87% 1.69G/1.95G [00:18<00:02, 91.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  88% 1.71G/1.95G [00:18<00:02, 108MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  89% 1.73G/1.95G [00:18<00:01, 122MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  90% 1.75G/1.95G [00:18<00:01, 134MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  91% 1.77G/1.95G [00:18<00:01, 146MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  92% 1.79G/1.95G [00:18<00:01, 145MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  93% 1.81G/1.95G [00:18<00:00, 154MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  94% 1.84G/1.95G [00:18<00:00, 157MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  95% 1.86G/1.95G [00:19<00:00, 164MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  96% 1.88G/1.95G [00:19<00:00, 170MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  98% 1.90G/1.95G [00:19<00:00, 171MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin:  99% 1.92G/1.95G [00:19<00:00, 173MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00008.bin: 100% 1.95G/1.95G [00:19<00:00, 99.4MB/s]\n",
            "Downloading shards:  25% 2/8 [00:37<01:54, 19.01s/it]\n",
            "pytorch_model-00003-of-00008.bin:   0% 0.00/1.98G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   1% 10.5M/1.98G [00:00<00:33, 59.6MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   1% 21.0M/1.98G [00:00<00:28, 68.4MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   2% 31.5M/1.98G [00:00<00:25, 75.6MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   3% 52.4M/1.98G [00:00<00:19, 99.0MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   4% 73.4M/1.98G [00:00<00:16, 113MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   5% 94.4M/1.98G [00:00<00:15, 122MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   6% 115M/1.98G [00:02<01:05, 28.5MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   7% 136M/1.98G [00:02<00:45, 40.2MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   8% 157M/1.98G [00:02<00:33, 54.6MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:   9% 178M/1.98G [00:03<00:27, 66.4MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  10% 199M/1.98G [00:03<00:22, 80.9MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  11% 220M/1.98G [00:03<00:17, 98.8MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  12% 241M/1.98G [00:03<00:14, 117MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  13% 262M/1.98G [00:03<00:13, 128MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  14% 283M/1.98G [00:03<00:12, 139MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  15% 304M/1.98G [00:03<00:11, 150MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  16% 325M/1.98G [00:03<00:10, 157MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  17% 346M/1.98G [00:03<00:09, 168MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  19% 367M/1.98G [00:04<00:09, 176MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  20% 388M/1.98G [00:04<00:08, 184MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  21% 409M/1.98G [00:04<00:08, 187MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  22% 430M/1.98G [00:04<00:08, 180MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  23% 451M/1.98G [00:05<00:38, 39.7MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  24% 472M/1.98G [00:06<00:29, 50.4MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  25% 493M/1.98G [00:06<00:23, 62.2MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  26% 514M/1.98G [00:06<00:19, 76.2MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  27% 535M/1.98G [00:07<00:40, 36.0MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  29% 566M/1.98G [00:07<00:26, 53.7MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  30% 587M/1.98G [00:07<00:20, 66.4MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  31% 608M/1.98G [00:08<00:17, 77.1MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  32% 629M/1.98G [00:08<00:15, 89.6MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  33% 661M/1.98G [00:08<00:11, 115MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  34% 682M/1.98G [00:08<00:10, 124MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  35% 703M/1.98G [00:08<00:10, 128MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  37% 724M/1.98G [00:08<00:12, 104MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  38% 755M/1.98G [00:09<00:09, 127MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  39% 776M/1.98G [00:09<00:08, 135MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  40% 797M/1.98G [00:09<00:08, 147MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  41% 818M/1.98G [00:09<00:07, 153MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  42% 839M/1.98G [00:09<00:07, 160MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  43% 860M/1.98G [00:09<00:06, 165MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  44% 881M/1.98G [00:09<00:06, 172MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  46% 902M/1.98G [00:09<00:06, 173MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  47% 923M/1.98G [00:09<00:05, 178MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  48% 944M/1.98G [00:12<00:43, 24.1MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  49% 975M/1.98G [00:12<00:27, 36.5MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  50% 996M/1.98G [00:13<00:22, 43.7MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  52% 1.03G/1.98G [00:13<00:15, 61.9MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  53% 1.06G/1.98G [00:13<00:11, 79.9MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  55% 1.08G/1.98G [00:13<00:09, 91.4MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  56% 1.10G/1.98G [00:13<00:08, 104MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  57% 1.12G/1.98G [00:13<00:07, 120MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  58% 1.14G/1.98G [00:13<00:06, 133MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  59% 1.16G/1.98G [00:13<00:05, 144MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  60% 1.18G/1.98G [00:14<00:05, 148MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  61% 1.21G/1.98G [00:14<00:04, 157MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  62% 1.23G/1.98G [00:14<00:04, 162MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  63% 1.25G/1.98G [00:14<00:04, 167MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  64% 1.27G/1.98G [00:14<00:04, 172MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  65% 1.29G/1.98G [00:14<00:03, 175MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  66% 1.31G/1.98G [00:14<00:03, 171MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  67% 1.33G/1.98G [00:17<00:29, 21.9MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  68% 1.35G/1.98G [00:17<00:21, 29.7MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  69% 1.37G/1.98G [00:18<00:16, 37.1MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  71% 1.41G/1.98G [00:18<00:10, 55.5MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  72% 1.43G/1.98G [00:18<00:08, 68.6MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  73% 1.45G/1.98G [00:18<00:06, 82.0MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  74% 1.47G/1.98G [00:18<00:05, 95.8MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  75% 1.49G/1.98G [00:18<00:04, 108MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  76% 1.51G/1.98G [00:18<00:03, 124MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  77% 1.53G/1.98G [00:18<00:03, 141MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  79% 1.56G/1.98G [00:19<00:02, 160MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  80% 1.58G/1.98G [00:19<00:03, 119MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  82% 1.61G/1.98G [00:19<00:02, 140MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  83% 1.64G/1.98G [00:19<00:02, 150MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  84% 1.66G/1.98G [00:19<00:02, 148MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  85% 1.68G/1.98G [00:19<00:01, 156MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  86% 1.70G/1.98G [00:19<00:01, 166MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  87% 1.72G/1.98G [00:20<00:01, 169MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  88% 1.74G/1.98G [00:20<00:01, 144MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  89% 1.76G/1.98G [00:21<00:05, 41.6MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  90% 1.78G/1.98G [00:21<00:03, 51.2MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  91% 1.80G/1.98G [00:22<00:02, 60.6MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  92% 1.82G/1.98G [00:22<00:02, 75.9MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  94% 1.86G/1.98G [00:22<00:01, 102MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  95% 1.88G/1.98G [00:22<00:00, 111MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  96% 1.90G/1.98G [00:22<00:00, 123MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  97% 1.92G/1.98G [00:22<00:00, 135MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  98% 1.94G/1.98G [00:22<00:00, 147MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin:  99% 1.96G/1.98G [00:22<00:00, 125MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00008.bin: 100% 1.98G/1.98G [00:23<00:00, 85.6MB/s]\n",
            "Downloading shards:  38% 3/8 [01:01<01:45, 21.02s/it]\n",
            "pytorch_model-00004-of-00008.bin:   0% 0.00/1.95G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   1% 10.5M/1.95G [00:00<00:53, 36.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   1% 21.0M/1.95G [00:00<00:43, 44.5MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   2% 31.5M/1.95G [00:00<00:35, 54.5MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   2% 41.9M/1.95G [00:00<00:30, 63.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   3% 52.4M/1.95G [00:00<00:26, 70.8MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   4% 73.4M/1.95G [00:01<00:19, 94.7MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   5% 94.4M/1.95G [00:01<00:16, 112MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   6% 115M/1.95G [00:01<00:14, 128MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   7% 136M/1.95G [00:01<00:13, 136MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   8% 157M/1.95G [00:01<00:12, 140MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:   9% 178M/1.95G [00:01<00:11, 151MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  10% 199M/1.95G [00:01<00:10, 161MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  11% 220M/1.95G [00:01<00:10, 168MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  12% 241M/1.95G [00:02<00:10, 167MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  13% 262M/1.95G [00:02<00:09, 177MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  15% 283M/1.95G [00:02<00:09, 176MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  16% 304M/1.95G [00:02<00:09, 181MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  17% 325M/1.95G [00:02<00:08, 183MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  18% 346M/1.95G [00:02<00:09, 176MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  19% 367M/1.95G [00:02<00:09, 174MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  20% 388M/1.95G [00:02<00:08, 182MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  21% 409M/1.95G [00:02<00:08, 183MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  22% 430M/1.95G [00:03<00:08, 182MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  23% 451M/1.95G [00:04<00:32, 46.6MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  24% 472M/1.95G [00:05<00:38, 38.6MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  26% 503M/1.95G [00:05<00:25, 57.7MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  27% 524M/1.95G [00:05<00:20, 70.6MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  28% 545M/1.95G [00:05<00:17, 82.4MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  29% 566M/1.95G [00:05<00:14, 96.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  30% 587M/1.95G [00:05<00:12, 110MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  31% 608M/1.95G [00:05<00:10, 123MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  32% 629M/1.95G [00:05<00:09, 139MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  34% 661M/1.95G [00:06<00:07, 162MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  35% 682M/1.95G [00:06<00:07, 164MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  36% 703M/1.95G [00:06<00:07, 168MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  37% 724M/1.95G [00:06<00:10, 121MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  39% 755M/1.95G [00:06<00:08, 146MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  40% 776M/1.95G [00:06<00:07, 156MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  41% 797M/1.95G [00:06<00:07, 162MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  42% 818M/1.95G [00:07<00:06, 167MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  43% 839M/1.95G [00:07<00:06, 167MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  44% 860M/1.95G [00:07<00:06, 171MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  45% 881M/1.95G [00:07<00:06, 172MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  46% 902M/1.95G [00:07<00:06, 172MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  47% 923M/1.95G [00:07<00:05, 176MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  48% 944M/1.95G [00:09<00:29, 34.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  50% 965M/1.95G [00:09<00:22, 44.1MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  51% 986M/1.95G [00:09<00:17, 53.5MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  52% 1.01G/1.95G [00:09<00:13, 68.5MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  53% 1.03G/1.95G [00:10<00:11, 83.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  54% 1.05G/1.95G [00:10<00:10, 84.5MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  55% 1.07G/1.95G [00:10<00:09, 89.0MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  56% 1.09G/1.95G [00:10<00:08, 96.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  57% 1.11G/1.95G [00:10<00:09, 90.1MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  58% 1.13G/1.95G [00:11<00:09, 86.5MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  59% 1.15G/1.95G [00:11<00:08, 93.7MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  60% 1.17G/1.95G [00:11<00:07, 96.6MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  61% 1.18G/1.95G [00:11<00:09, 83.6MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  61% 1.20G/1.95G [00:11<00:09, 76.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  62% 1.21G/1.95G [00:12<00:11, 65.8MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  62% 1.22G/1.95G [00:12<00:10, 69.8MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  63% 1.23G/1.95G [00:12<00:10, 71.3MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  64% 1.24G/1.95G [00:12<00:11, 62.5MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  65% 1.26G/1.95G [00:12<00:08, 84.8MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  66% 1.28G/1.95G [00:13<00:07, 92.0MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  67% 1.30G/1.95G [00:13<00:05, 109MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  68% 1.32G/1.95G [00:13<00:04, 126MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  69% 1.34G/1.95G [00:13<00:04, 138MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  70% 1.36G/1.95G [00:13<00:04, 145MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  71% 1.38G/1.95G [00:13<00:03, 149MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  72% 1.41G/1.95G [00:13<00:03, 154MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  73% 1.43G/1.95G [00:13<00:03, 156MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  74% 1.45G/1.95G [00:14<00:06, 82.4MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  75% 1.47G/1.95G [00:14<00:04, 97.0MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  77% 1.49G/1.95G [00:14<00:04, 114MB/s] \u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  78% 1.52G/1.95G [00:14<00:03, 139MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  79% 1.54G/1.95G [00:14<00:02, 142MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  80% 1.56G/1.95G [00:15<00:02, 147MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  81% 1.58G/1.95G [00:15<00:02, 159MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  83% 1.61G/1.95G [00:15<00:02, 164MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  84% 1.64G/1.95G [00:15<00:02, 148MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  85% 1.66G/1.95G [00:15<00:01, 153MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  86% 1.68G/1.95G [00:15<00:01, 161MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  88% 1.71G/1.95G [00:15<00:01, 173MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  89% 1.73G/1.95G [00:16<00:01, 179MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  90% 1.75G/1.95G [00:16<00:01, 179MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  91% 1.77G/1.95G [00:16<00:01, 170MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  92% 1.79G/1.95G [00:16<00:00, 167MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  93% 1.81G/1.95G [00:16<00:00, 168MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  94% 1.84G/1.95G [00:16<00:00, 166MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  95% 1.86G/1.95G [00:16<00:00, 168MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  96% 1.88G/1.95G [00:17<00:00, 104MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  98% 1.90G/1.95G [00:19<00:01, 25.7MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin:  99% 1.93G/1.95G [00:19<00:00, 39.2MB/s]\u001b[A\n",
            "pytorch_model-00004-of-00008.bin: 100% 1.95G/1.95G [00:19<00:00, 98.6MB/s]\n",
            "Downloading shards:  50% 4/8 [01:20<01:22, 20.57s/it]\n",
            "pytorch_model-00005-of-00008.bin:   0% 0.00/1.98G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   1% 10.5M/1.98G [00:00<00:49, 40.2MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   1% 21.0M/1.98G [00:00<00:39, 50.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   2% 31.5M/1.98G [00:00<00:32, 60.1MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   2% 41.9M/1.98G [00:00<00:27, 70.2MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   3% 62.9M/1.98G [00:00<00:21, 90.7MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   4% 83.9M/1.98G [00:00<00:17, 110MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   5% 105M/1.98G [00:01<00:14, 130MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   6% 126M/1.98G [00:01<00:12, 146MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   7% 147M/1.98G [00:01<00:11, 157MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:   8% 168M/1.98G [00:01<00:11, 160MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  10% 189M/1.98G [00:01<00:10, 170MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  11% 210M/1.98G [00:01<00:10, 169MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  12% 231M/1.98G [00:01<00:10, 168MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  13% 252M/1.98G [00:01<00:10, 173MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  14% 273M/1.98G [00:02<00:09, 172MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  15% 294M/1.98G [00:02<00:09, 172MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  16% 315M/1.98G [00:02<00:09, 175MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  17% 336M/1.98G [00:02<00:09, 173MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  18% 357M/1.98G [00:02<00:09, 164MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  19% 377M/1.98G [00:02<00:09, 167MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  20% 398M/1.98G [00:02<00:09, 168MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  21% 419M/1.98G [00:02<00:08, 175MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  22% 440M/1.98G [00:04<00:43, 35.6MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  24% 472M/1.98G [00:04<00:28, 53.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  25% 493M/1.98G [00:04<00:24, 60.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  26% 514M/1.98G [00:05<00:19, 74.9MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  27% 535M/1.98G [00:05<00:15, 91.6MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  28% 556M/1.98G [00:05<00:13, 107MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  29% 577M/1.98G [00:05<00:11, 120MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  30% 598M/1.98G [00:05<00:10, 132MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  31% 619M/1.98G [00:05<00:09, 146MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  32% 640M/1.98G [00:05<00:08, 154MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  33% 661M/1.98G [00:05<00:08, 162MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  34% 682M/1.98G [00:05<00:07, 168MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  35% 703M/1.98G [00:06<00:07, 171MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  37% 724M/1.98G [00:06<00:07, 176MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  38% 744M/1.98G [00:06<00:07, 176MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  39% 765M/1.98G [00:06<00:06, 179MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  40% 786M/1.98G [00:06<00:06, 179MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  41% 807M/1.98G [00:06<00:06, 181MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  42% 828M/1.98G [00:07<00:11, 99.7MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  43% 849M/1.98G [00:08<00:31, 35.4MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  44% 870M/1.98G [00:08<00:24, 45.4MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  45% 891M/1.98G [00:08<00:18, 58.1MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  47% 923M/1.98G [00:08<00:12, 81.9MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  48% 944M/1.98G [00:09<00:10, 95.2MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  49% 965M/1.98G [00:09<00:10, 100MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  50% 986M/1.98G [00:09<00:09, 108MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  51% 1.01G/1.98G [00:09<00:08, 111MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  52% 1.03G/1.98G [00:09<00:07, 125MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  53% 1.06G/1.98G [00:09<00:06, 150MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  55% 1.08G/1.98G [00:10<00:05, 155MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  56% 1.10G/1.98G [00:10<00:06, 145MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  57% 1.12G/1.98G [00:10<00:05, 149MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  58% 1.14G/1.98G [00:10<00:06, 126MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  59% 1.16G/1.98G [00:10<00:05, 137MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  60% 1.18G/1.98G [00:10<00:05, 148MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  61% 1.21G/1.98G [00:10<00:05, 150MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  62% 1.23G/1.98G [00:11<00:07, 100MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  63% 1.25G/1.98G [00:11<00:09, 77.4MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  64% 1.26G/1.98G [00:11<00:09, 75.9MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  64% 1.27G/1.98G [00:12<00:10, 68.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  65% 1.28G/1.98G [00:12<00:10, 69.8MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  65% 1.29G/1.98G [00:12<00:09, 72.9MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  66% 1.31G/1.98G [00:12<00:08, 76.4MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  67% 1.33G/1.98G [00:12<00:07, 85.8MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  68% 1.34G/1.98G [00:12<00:07, 87.5MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  68% 1.35G/1.98G [00:13<00:10, 62.4MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  69% 1.37G/1.98G [00:13<00:07, 81.8MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  70% 1.39G/1.98G [00:13<00:06, 90.5MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  71% 1.41G/1.98G [00:13<00:07, 76.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  72% 1.42G/1.98G [00:14<00:14, 37.7MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  72% 1.43G/1.98G [00:15<00:19, 27.8MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  73% 1.45G/1.98G [00:15<00:12, 42.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  75% 1.48G/1.98G [00:15<00:07, 68.2MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  76% 1.51G/1.98G [00:15<00:05, 93.1MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  77% 1.53G/1.98G [00:15<00:04, 106MB/s] \u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  78% 1.55G/1.98G [00:15<00:03, 118MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  79% 1.57G/1.98G [00:15<00:03, 134MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  81% 1.59G/1.98G [00:16<00:02, 147MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  82% 1.61G/1.98G [00:16<00:02, 156MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  83% 1.64G/1.98G [00:16<00:02, 159MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  84% 1.66G/1.98G [00:16<00:01, 166MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  85% 1.68G/1.98G [00:16<00:01, 170MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  86% 1.70G/1.98G [00:16<00:01, 170MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  87% 1.72G/1.98G [00:16<00:01, 177MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  88% 1.74G/1.98G [00:16<00:01, 175MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  89% 1.76G/1.98G [00:17<00:01, 179MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  90% 1.78G/1.98G [00:17<00:01, 177MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  91% 1.80G/1.98G [00:17<00:00, 177MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  92% 1.82G/1.98G [00:17<00:00, 181MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  93% 1.85G/1.98G [00:17<00:00, 181MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  94% 1.87G/1.98G [00:19<00:03, 29.7MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  95% 1.89G/1.98G [00:19<00:02, 39.4MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  96% 1.91G/1.98G [00:19<00:01, 51.7MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  97% 1.93G/1.98G [00:19<00:00, 66.6MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin:  99% 1.95G/1.98G [00:20<00:00, 82.0MB/s]\u001b[A\n",
            "pytorch_model-00005-of-00008.bin: 100% 1.98G/1.98G [00:20<00:00, 97.9MB/s]\n",
            "Downloading shards:  62% 5/8 [01:41<01:01, 20.49s/it]\n",
            "pytorch_model-00006-of-00008.bin:   0% 0.00/1.95G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:   1% 10.5M/1.95G [00:00<00:20, 95.3MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:   2% 31.5M/1.95G [00:00<00:16, 118MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00008.bin:   3% 52.4M/1.95G [00:00<00:14, 135MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:   4% 73.4M/1.95G [00:00<00:13, 143MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:   5% 105M/1.95G [00:00<00:10, 170MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00008.bin:   7% 136M/1.95G [00:00<00:09, 187MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:   9% 168M/1.95G [00:00<00:09, 194MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  10% 199M/1.95G [00:01<00:08, 196MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  12% 231M/1.95G [00:01<00:08, 203MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  13% 262M/1.95G [00:01<00:07, 215MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  15% 294M/1.95G [00:01<00:07, 221MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  17% 325M/1.95G [00:01<00:07, 226MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  18% 357M/1.95G [00:01<00:07, 226MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  20% 388M/1.95G [00:01<00:06, 230MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  22% 419M/1.95G [00:02<00:06, 231MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  23% 451M/1.95G [00:02<00:06, 225MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  25% 482M/1.95G [00:02<00:06, 223MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  26% 514M/1.95G [00:02<00:06, 226MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  28% 545M/1.95G [00:04<00:27, 50.1MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  29% 566M/1.95G [00:04<00:23, 58.8MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  30% 587M/1.95G [00:04<00:20, 66.3MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  32% 619M/1.95G [00:04<00:15, 87.8MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  33% 650M/1.95G [00:04<00:11, 110MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  35% 682M/1.95G [00:05<00:10, 125MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  36% 703M/1.95G [00:05<00:09, 134MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  37% 724M/1.95G [00:05<00:08, 144MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  38% 744M/1.95G [00:05<00:08, 150MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  39% 765M/1.95G [00:05<00:07, 162MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  41% 797M/1.95G [00:05<00:06, 186MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  43% 828M/1.95G [00:05<00:05, 200MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  44% 860M/1.95G [00:05<00:05, 200MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  46% 891M/1.95G [00:06<00:05, 207MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  47% 923M/1.95G [00:08<00:25, 40.1MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  48% 944M/1.95G [00:09<00:30, 32.4MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  50% 975M/1.95G [00:09<00:21, 45.5MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  51% 996M/1.95G [00:09<00:17, 55.3MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  52% 1.02G/1.95G [00:09<00:14, 63.4MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  54% 1.05G/1.95G [00:09<00:10, 84.4MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  55% 1.08G/1.95G [00:10<00:07, 109MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  57% 1.10G/1.95G [00:10<00:07, 117MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  58% 1.12G/1.95G [00:10<00:06, 122MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  59% 1.14G/1.95G [00:10<00:06, 125MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  60% 1.16G/1.95G [00:10<00:06, 123MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  61% 1.20G/1.95G [00:10<00:05, 146MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  63% 1.23G/1.95G [00:10<00:04, 167MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  64% 1.25G/1.95G [00:11<00:04, 168MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  65% 1.27G/1.95G [00:11<00:04, 166MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  66% 1.29G/1.95G [00:11<00:04, 161MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  67% 1.31G/1.95G [00:11<00:03, 164MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  68% 1.33G/1.95G [00:12<00:13, 44.5MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  70% 1.35G/1.95G [00:14<00:21, 27.1MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  71% 1.37G/1.95G [00:14<00:15, 36.1MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  72% 1.39G/1.95G [00:14<00:12, 44.6MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  73% 1.42G/1.95G [00:14<00:09, 57.9MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  74% 1.45G/1.95G [00:14<00:06, 81.8MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  76% 1.48G/1.95G [00:14<00:04, 107MB/s] \u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  77% 1.50G/1.95G [00:15<00:03, 117MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  78% 1.52G/1.95G [00:15<00:03, 129MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  79% 1.54G/1.95G [00:15<00:02, 143MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  81% 1.57G/1.95G [00:15<00:02, 161MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  82% 1.60G/1.95G [00:15<00:01, 178MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  84% 1.64G/1.95G [00:15<00:01, 196MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  86% 1.67G/1.95G [00:15<00:01, 204MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  87% 1.70G/1.95G [00:16<00:01, 213MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  89% 1.73G/1.95G [00:16<00:00, 219MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  91% 1.76G/1.95G [00:16<00:00, 209MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  92% 1.79G/1.95G [00:16<00:00, 211MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  94% 1.82G/1.95G [00:16<00:00, 213MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  95% 1.86G/1.95G [00:18<00:01, 54.3MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  97% 1.89G/1.95G [00:18<00:00, 71.2MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin:  98% 1.91G/1.95G [00:18<00:00, 80.8MB/s]\u001b[A\n",
            "pytorch_model-00006-of-00008.bin: 100% 1.95G/1.95G [00:18<00:00, 104MB/s] \n",
            "Downloading shards:  75% 6/8 [02:00<00:39, 19.93s/it]\n",
            "pytorch_model-00007-of-00008.bin:   0% 0.00/1.98G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:   1% 21.0M/1.98G [00:00<00:10, 191MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:   2% 41.9M/1.98G [00:00<00:09, 197MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:   4% 73.4M/1.98G [00:00<00:09, 210MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:   5% 105M/1.98G [00:00<00:08, 209MB/s] \u001b[A\n",
            "pytorch_model-00007-of-00008.bin:   6% 126M/1.98G [00:00<00:08, 208MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:   8% 157M/1.98G [00:00<00:08, 211MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  10% 189M/1.98G [00:00<00:08, 211MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  11% 220M/1.98G [00:01<00:07, 224MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  13% 252M/1.98G [00:01<00:07, 228MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  14% 283M/1.98G [00:01<00:07, 232MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  16% 315M/1.98G [00:01<00:07, 218MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  17% 346M/1.98G [00:01<00:07, 211MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  19% 377M/1.98G [00:01<00:07, 215MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  21% 409M/1.98G [00:01<00:07, 214MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  22% 440M/1.98G [00:02<00:07, 216MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  24% 472M/1.98G [00:02<00:06, 222MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  25% 503M/1.98G [00:02<00:06, 231MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  27% 535M/1.98G [00:02<00:06, 221MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  29% 566M/1.98G [00:02<00:06, 225MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  30% 598M/1.98G [00:02<00:05, 232MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  32% 629M/1.98G [00:02<00:05, 225MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  33% 661M/1.98G [00:03<00:06, 211MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  35% 692M/1.98G [00:03<00:06, 213MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  37% 724M/1.98G [00:03<00:05, 210MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  38% 755M/1.98G [00:05<00:27, 44.7MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  40% 786M/1.98G [00:05<00:20, 57.2MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  41% 807M/1.98G [00:05<00:18, 64.6MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  42% 839M/1.98G [00:05<00:13, 84.5MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  44% 870M/1.98G [00:05<00:10, 107MB/s] \u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  45% 891M/1.98G [00:06<00:09, 119MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  47% 923M/1.98G [00:06<00:07, 139MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  48% 944M/1.98G [00:06<00:07, 148MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  49% 965M/1.98G [00:06<00:06, 155MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  50% 986M/1.98G [00:06<00:06, 166MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  51% 1.02G/1.98G [00:06<00:05, 180MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  53% 1.05G/1.98G [00:06<00:04, 198MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  55% 1.08G/1.98G [00:06<00:04, 205MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  56% 1.11G/1.98G [00:07<00:04, 215MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  58% 1.14G/1.98G [00:07<00:03, 220MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  59% 1.17G/1.98G [00:07<00:03, 230MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  61% 1.21G/1.98G [00:07<00:03, 231MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  62% 1.24G/1.98G [00:07<00:03, 235MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  64% 1.27G/1.98G [00:07<00:03, 233MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  66% 1.30G/1.98G [00:07<00:02, 230MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  67% 1.33G/1.98G [00:08<00:02, 227MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  69% 1.36G/1.98G [00:08<00:02, 220MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  70% 1.39G/1.98G [00:08<00:02, 219MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  72% 1.43G/1.98G [00:08<00:04, 112MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  73% 1.45G/1.98G [00:10<00:12, 43.2MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  74% 1.47G/1.98G [00:10<00:10, 47.0MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  75% 1.49G/1.98G [00:10<00:08, 57.9MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  76% 1.51G/1.98G [00:11<00:06, 71.6MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  78% 1.54G/1.98G [00:11<00:04, 96.2MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  79% 1.56G/1.98G [00:11<00:03, 111MB/s] \u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  80% 1.58G/1.98G [00:11<00:03, 121MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  82% 1.61G/1.98G [00:11<00:02, 144MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  83% 1.65G/1.98G [00:11<00:02, 163MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  84% 1.67G/1.98G [00:11<00:01, 171MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  86% 1.70G/1.98G [00:11<00:01, 184MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  87% 1.73G/1.98G [00:12<00:01, 197MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  89% 1.76G/1.98G [00:12<00:01, 204MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  91% 1.79G/1.98G [00:12<00:00, 212MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  92% 1.82G/1.98G [00:12<00:00, 215MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  94% 1.86G/1.98G [00:12<00:00, 224MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  95% 1.89G/1.98G [00:12<00:00, 232MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  97% 1.92G/1.98G [00:12<00:00, 232MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin:  99% 1.95G/1.98G [00:12<00:00, 237MB/s]\u001b[A\n",
            "pytorch_model-00007-of-00008.bin: 100% 1.98G/1.98G [00:13<00:00, 150MB/s]\n",
            "Downloading shards:  88% 7/8 [02:13<00:17, 17.77s/it]\n",
            "pytorch_model-00008-of-00008.bin:   0% 0.00/816M [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:   4% 31.5M/816M [00:00<00:03, 243MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:   8% 62.9M/816M [00:00<00:03, 247MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  12% 94.4M/816M [00:00<00:02, 250MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  15% 126M/816M [00:00<00:02, 241MB/s] \u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  19% 157M/816M [00:00<00:02, 222MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  23% 189M/816M [00:00<00:02, 221MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  27% 220M/816M [00:00<00:02, 227MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  31% 252M/816M [00:01<00:02, 225MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  35% 283M/816M [00:01<00:02, 234MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  39% 315M/816M [00:01<00:02, 236MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  42% 346M/816M [00:01<00:02, 225MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  46% 377M/816M [00:01<00:02, 201MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  49% 398M/816M [00:01<00:02, 196MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  51% 419M/816M [00:01<00:02, 190MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  54% 440M/816M [00:02<00:02, 186MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  57% 461M/816M [00:02<00:01, 189MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  59% 482M/816M [00:02<00:01, 192MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  63% 514M/816M [00:02<00:01, 207MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  67% 545M/816M [00:02<00:01, 212MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  71% 577M/816M [00:02<00:01, 205MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  73% 598M/816M [00:02<00:01, 191MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  76% 619M/816M [00:02<00:01, 185MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  78% 640M/816M [00:03<00:01, 174MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  81% 661M/816M [00:03<00:00, 177MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  84% 682M/816M [00:03<00:00, 182MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  87% 713M/816M [00:03<00:00, 195MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  91% 744M/816M [00:03<00:00, 204MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  94% 765M/816M [00:03<00:00, 199MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin:  96% 786M/816M [00:03<00:00, 195MB/s]\u001b[A\n",
            "pytorch_model-00008-of-00008.bin: 100% 816M/816M [00:04<00:00, 203MB/s]\n",
            "Downloading shards: 100% 8/8 [02:17<00:00, 17.21s/it]\n",
            "Loading checkpoint shards: 100% 8/8 [01:00<00:00,  7.54s/it]\n",
            "generation_config.json: 100% 111/111 [00:00<00:00, 577kB/s]\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:48:02\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m332\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:48:02\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m340\u001b[0m - \u001b[1mpreparing peft model...\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:48:02\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m391\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
            "Running tokenizer on train dataset: 100% 244/244 [00:00<00:00, 2375.42 examples/s]\n",
            "Grouping texts in chunks of 1024 (num_proc=4): 100% 244/244 [00:00<00:00, 729.00 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:48:03\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "{'train_runtime': 156.8134, 'train_samples_per_second': 0.255, 'train_steps_per_second': 0.064, 'train_loss': 1.0412042617797852, 'epoch': 1.0}\n",
            "100% 10/10 [02:36<00:00, 15.68s/it]\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:50:41\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m541\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-04-04 22:50:41\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1mPushing model to hub...\u001b[0m\n",
            "adapter_model.safetensors:   0% 0.00/27.3M [00:00<?, ?B/s]\n",
            "adapter_model.safetensors:   0% 0.00/27.3M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "optimizer.pt:   0% 0.00/54.6M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Upload 10 LFS files:   0% 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:   0% 0.00/888 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   0% 16.4k/27.3M [00:00<07:11, 63.1kB/s]\n",
            "\n",
            "optimizer.pt:   0% 16.4k/54.6M [00:00<15:11, 59.9kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin: 100% 888/888 [00:00<00:00, 3.15kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 50.1kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  21% 5.80M/27.3M [00:00<00:01, 18.6MB/s]\n",
            "adapter_model.safetensors:  16% 4.26M/27.3M [00:00<00:01, 13.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin: 100% 888/888 [00:00<00:00, 2.21kB/s]\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 32.2kB/s]\n",
            "adapter_model.safetensors:  35% 9.49M/27.3M [00:00<00:01, 17.5MB/s]\n",
            "adapter_model.safetensors:  30% 8.08M/27.3M [00:00<00:01, 15.2MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  19% 10.6M/54.6M [00:00<00:02, 17.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  51% 14.0M/27.3M [00:00<00:00, 23.0MB/s]\n",
            "adapter_model.safetensors:  43% 11.7M/27.3M [00:00<00:00, 18.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 7.25kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  62% 16.8M/27.3M [00:01<00:00, 15.0MB/s]\n",
            "\n",
            "optimizer.pt:  29% 16.0M/54.6M [00:01<00:02, 14.4MB/s]\u001b[A\u001b[A\n",
            "training_args.bin: 100% 4.98k/4.98k [00:00<00:00, 48.9kB/s]\n",
            "adapter_model.safetensors:  97% 26.4M/27.3M [00:01<00:00, 29.1MB/s]\n",
            "\n",
            "optimizer.pt:  46% 25.3M/54.6M [00:01<00:01, 25.4MB/s]\u001b[A\u001b[A\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 942kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model:   0% 0.00/493k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors: 100% 27.3M/27.3M [00:01<00:00, 18.7MB/s]\n",
            "\n",
            "\n",
            "training_args.bin: 100% 4.98k/4.98k [00:00<00:00, 31.8kB/s]\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 1.44MB/s]\n",
            "adapter_model.safetensors: 100% 27.3M/27.3M [00:01<00:00, 14.2MB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:  59% 32.2M/54.6M [00:02<00:01, 11.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Upload 10 LFS files:  20% 2/10 [00:02<00:08,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  80% 43.6M/54.6M [00:02<00:00, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt: 100% 54.6M/54.6M [00:03<00:00, 18.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Upload 10 LFS files: 100% 10/10 [00:03<00:00,  2.83it/s]\n"
          ]
        }
      ],
      "source": [
        "!autotrain llm \\\n",
        "--train \\\n",
        "--model ${MODEL_NAME} \\\n",
        "--project-name ${PROJECT_NAME} \\\n",
        "--data-path data/ \\\n",
        "--text-column text \\\n",
        "--lr ${LEARNING_RATE} \\\n",
        "--batch-size ${BATCH_SIZE} \\\n",
        "--epochs ${NUM_EPOCHS} \\\n",
        "--block-size ${BLOCK_SIZE} \\\n",
        "--warmup-ratio ${WARMUP_RATIO} \\\n",
        "--lora-r ${LORA_R} \\\n",
        "--lora-alpha ${LORA_ALPHA} \\\n",
        "--lora-dropout ${LORA_DROPOUT} \\\n",
        "--weight-decay ${WEIGHT_DECAY} \\\n",
        "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
        "--quantization ${QUANTIZATION} \\\n",
        "--mixed-precision ${MIXED_PRECISION} \\\n",
        "$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n",
        "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}